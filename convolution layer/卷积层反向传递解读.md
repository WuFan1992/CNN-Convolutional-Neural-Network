# 卷积层反向传递解读

## 卷积层反向传递实现目标

1.反向传递误差

2.计算w和b的梯度改变量，在计算的时候会用到反向传递误差

3.更新w和b


## 误差的正向传递
原本卷积神经网络传递的是z，a, 但是同时误差也会随着卷积层传递，也就是说误差也做着卷积运算，从l-1 层通过卷积到 l 层
![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/14.png)


## 误差的反向传递
我们已知 l 层的误差，要求 l-1 层的误差，怎么办，先来看误差的定义式，这里使用了链式法则
![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/16.PNG)

我们先求左边的式子，E 关于 l-1 层的 a 的偏导数

很难看出联系，反向传递是为了连接 l-1 层和 l 层，唯一能连接这两层的公式是z = wx + b

![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/17.PNG)

所以为了运用这个唯一的联系，我们把左边的式子再使用一次链式法则

![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/18.PNG)

这里我们选取的只是（1,1）位置的激励，针对（1,2），（2,1），（2,2）的激励，也是一样的方法

针对（1,2）位置的

![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/19.PNG)

![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/20.PNG)

针对（2,2）位置的


![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/21.PNG)

![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/22.PNG)


**归纳**
根据最终结果，我们发现，反向传递相当于是在 l 层最外面补一圈 0 然后， 把卷积核filter 旋转180 度，再进行卷积运算，如下图

![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/23.png)

现在我们算完了左边，开始算右边，其实右边就是f（z）的导数

![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/25.PNG)

把左边右边综合起来，就是这个式子，和SGD 中的表达式一致

![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/26.PNG)
