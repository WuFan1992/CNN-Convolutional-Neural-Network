# 卷积层反向传递解读

## 卷积层反向传递实现目标

1.反向传递误差

2.计算w和b的梯度改变量，在计算的时候会用到反向传递误差

3.更新w和b


## 误差的正向传递
原本卷积神经网络传递的是z，a, 但是同时误差也会随着卷积层传递，也就是说误差也做着卷积运算，从l-1 层通过卷积到 l 层
![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/14.png)


## 误差的反向传递
我们已知 l 层的误差，要求 l-1 层的误差，怎么办，先来看误差的定义式，这里使用了链式法则
![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/16.PNG)

我们先求左边的式子，E 关于 l-1 层的 a 的偏导数

很难看出联系，反向传递是为了连接 l-1 层和 l 层，唯一能连接这两层的公式是z = wx + b

![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/17.PNG)

所以为了运用这个唯一的联系，我们把左边的式子再使用一次链式法则

![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/18.PNG)

这里我们选取的只是（1,1）位置的激励，针对（1,2），（2,1），（2,2）的激励，也是一样的方法

针对（1,2）位置的

![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/19.PNG)

![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/20.PNG)

针对（2,2）位置的


![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/21.PNG)

![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/22.PNG)


**归纳**
根据最终结果，我们发现，反向传递相当于是在 l 层最外面补一圈 0 然后， 把卷积核filter 旋转180 度，再进行卷积运算，如下图

![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/23.png)

现在我们算完了左边，开始算右边，其实右边就是f（z）的导数

![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/25.PNG)

把左边右边综合起来，就是这个式子，和SGD 中的表达式一致

![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/26.PNG)

**以上的例子是步长为1 ，深度为1 ， filter 的数量为1 ，下面我们讨论 都不为1 的情况**


### 步长不为1

![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/27.png)

和步长为1 相比，步长为 2 时，长和宽每隔两位就丢失一个数据，为了恢复，我们从步长为2 恢复到步长为1 时，在缺失位补上 0

根据上图，代码如下

```
# recover the array from stride larger than 1 to stride 1

def expand_stride(self,sensitive_array):

    depth = sensitive_array.shape[0]

    expand_width = self.input_width - self.filter_width + 2*self.zero_padding +1

    expand_height = self.input_height - self.filter_height + 2*self.zero_padding +1

    expand_array = np.zeros((depth,expand_height,expand_width))


    for i in range(self.output_height):
        for j in range(self.output_width):

            i_pos = i*self.stride
            j_pos = j*self.stride

            expand_array[;,i_pos-1,j_pos-1]= sensitive_array[:,i,j]

    return expand_array
    
 ```
