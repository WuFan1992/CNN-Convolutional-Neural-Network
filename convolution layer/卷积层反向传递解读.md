# 卷积层反向传递解读

## 卷积层反向传递实现目标

1.反向传递误差

2.计算w和b的梯度改变量，在计算的时候会用到反向传递误差

3.更新w和b


## 误差的正向传递
原本卷积神经网络传递的是z，a, 但是同时误差也会随着卷积层传递，也就是说误差也做着卷积运算，从l-1 层通过卷积到 l 层
![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/14.png)


## 误差的反向传递
我们已知 l 层的误差，要求 l-1 层的误差，怎么办，先来看误差的定义式，这里使用了链式法则
![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/16.PNG)

我们先求左边的式子，E 关于 l-1 层的 a 的偏导数

很难看出联系，反向传递是为了连接 l-1 层和 l 层，唯一能连接这两层的公式是z = wx + b

![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/17.PNG)

所以为了运用这个唯一的联系，我们把左边的式子再使用一次链式法则

![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/18.PNG)

