# 卷积层反向传递解读

## 卷积层反向传递实现目标

1.反向传递误差

2.计算w和b的梯度改变量，在计算的时候会用到反向传递误差

3.更新w和b


## 误差的正向传递
原本卷积神经网络传递的是z，a, 但是同时误差也会随着卷积层传递，也就是说误差也做着卷积运算，从l-1 层通过卷积到 l 层
![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/14.png)


## 误差的反向传递
我们已知 l 层的误差，要求 l-1 层的误差，怎么办，先来看误差的定义式，这里使用了链式法则
![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/16.PNG)

我们先求左边的式子，E 关于 l-1 层的 a 的偏导数

很难看出联系，反向传递是为了连接 l-1 层和 l 层，唯一能连接这两层的公式是z = wx + b

![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/17.PNG)

所以为了运用这个唯一的联系，我们把左边的式子再使用一次链式法则

![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/18.PNG)

这里我们选取的只是（1,1）位置的激励，针对（1,2），（2,1），（2,2）的激励，也是一样的方法

针对（1,2）位置的

![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/19.PNG)

![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/20.PNG)

针对（2,2）位置的


![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/21.PNG)

![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/22.PNG)


**归纳**
根据最终结果，我们发现，反向传递相当于是在 l 层最外面补一圈 0 然后， 把卷积核filter 旋转180 度，再进行卷积运算，如下图

![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/23.png)

现在我们算完了左边，开始算右边，其实右边就是f（z）的导数

![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/25.PNG)

把左边右边综合起来，就是这个式子，和SGD 中的表达式一致

![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/26.PNG)

**以上的例子是步长为1 ，深度为1 ， filter 的数量为1 ，下面我们讨论 都不为1 的情况**


### 步长不为1

![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/27.png)

和步长为1 相比，步长为 2 时，长和宽每隔两位就丢失一个数据，为了恢复，我们从步长为2 恢复到步长为1 时，在缺失位补上 0

根据上图，代码如下

```
# recover the array from stride larger than 1 to stride 1

def expand_stride(self,sensitive_array):

    depth = sensitive_array.shape[0]

    expand_width = self.input_width - self.filter_width + 2*self.zero_padding +1

    expand_height = self.input_height - self.filter_height + 2*self.zero_padding +1

    expand_array = np.zeros((depth,expand_height,expand_width))


    for i in range(self.output_height):
        for j in range(self.output_width):

            i_pos = i*self.stride
            j_pos = j*self.stride

            expand_array[;,i_pos-1,j_pos-1]= sensitive_array[:,i,j]

    return expand_array
    
 ```
在反向传递时，我们依然把l-1 层作为input 把 l 层作为output，所以　这个函数实际上是直接对　output 进行操作
expand_width 和 expand_height 都是 stride = 1时，output 的宽和高，计算这两个值不需要涉及sensitive_map

## 深度不为1
这个深度指的是l-1 层，也就是输入层的深度，这个深度和filter 深度一致，为什么一致，看这个正向传播图

![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/matrice1.gif)

![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/28.png)



根据上图，layer i 与 filter 的d1 卷积得到 layer i - 1 层的 d1
layer i 与 filter 的 d2 卷积得到 layer i-1 层 d2 



## filter 个数不为 1


还是这张正向传播图 ，我们从右往左看

![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/matrice1.gif)

右边输出层第一层，与W1的第一层卷积，加上右边输出层第二层与w0 的第一层的卷积，得到左边输入层第一层的误差

右边输出层第一层，与W1的第二层卷积，加上右边输出层第二层与w0 的第二层的卷积，得到左边输入层第二层的误差

右边输出层第三层，与W1的第三层卷积，加上右边输出层第二层与w0 的第三层的卷积，得到左边输入层第三层的误差

## 代码思路 
**代码目标**
已知 i 层的 误差矩阵，也就是sensitive map, 求 i-1 层的误差矩阵

**具体实现方式**

1. 根据步长stride，将第 i 层的误差矩阵还原成stride 为 1 的情况
2. 根据Input 和 output 的关系，确定从output 还原成Input 时，周围需要补充多少个0 ，也就是zero_padding
3. 最后将filter 旋转180 度，然后与layer i 层卷积，得到layer i-1 层的误差矩阵sensitive map

