# 神经网络卷积层概况     
## 卷积层图

![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/general.png)  
图中有两个卷积层，同时我们把卷积核称之为filter

第一个convolution layer 由input image 变成 3 feature maps ，需要3个filter  
第二个convolution layer 由 3个feature maps 变成 5个feature maps ,需要5个filter  
总结： 需要几个feature maps 就需要几个 filer. 原因如下图：  
![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/matrice1.gif)  
由上图看到，我们需要2个feature maps输出，就需要2个filter W0 和 W1


## 矩阵卷积怎么算   
**原则** 把卷积核贴着输入矩阵，一位一位对着乘起来，然后所有值相加，接着再把卷积核位移，重复同样的运算  
![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/1.png)  
![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/2.png)
![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/3.gif)  
上面是卷积核位移步长为 1 的， 下面是步长为 2    
![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/4.png)    
![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/5.png)    
![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/6.png)    
![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/7.png)    

**卷积后矩阵大小的计算**      
**公式**     
![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/8.PNG)    

公式里，W1 是 卷积前矩阵的宽， F 是卷积核filter的宽 ， P 是指外圈补了几层 0 ， S 是卷积核移动的步长 ， W2就是卷积后矩阵的宽  
H 也是一样的道理
在上面的例子中，W1 = 5      F = 3     P= 0     S = 2    
![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/9.PNG)    

计算卷积后矩阵大小的函数 calculate_output_size 如下    
```
def calculate_output_size(input_size,filter_size,zero_padding,stride):
    return (input_size - filter_size + 2*zero_padding)/stride +1
```
计算二维卷积的函数 conv_2d 如下
```
def conv_2d(input_array,kernel_array,output_array,stride,zero_padding,bias):
    
    input_array_size=input_array.shape[0]
    kernel_array_size=kernel_array.shape[0]
    
    output_array_size=calculate_output_size(input_array_size,kernel_array_size,zero_padding,stride)
    for i in range(output_array_size):
        for j in range(output_array_size):
            output_array[i][j]=np.sum(input_array[i+kernel_array_size:j+kernel_array_size]*kernel)+bias
    
```
## 卷积层的正向传递    
![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/matrice1.gif)   


**关键**    
input 是7x7x3 矩阵，深度为3 那么filter 的深度也要求一定是3 ， output 的深度取决于 filter 的个数，图中有2个filter ,所以output的深度是2

### 初始化类          
我们需要两个类：**ConvoLayer** 和 **Filter**

**ConvoLayer** 
用来存储 input ， 卷积层，output 的基本信息，主要涉及维度和初始化
```
class ConvoLayer(object):

    def _init_(self, input_width,input_height,input_depth,filter_height,filter_width,filter_number,zero_padding,stride,learning_rate,activators):

        '''
        ConvoLayer has 3 parts:
            1. input array  ----- height width depth
            2. filter       ----- height width depth(the same as input array),number
            3. output array ----- height width depth (the same as filter number)


        we also need to combine the N filter together to forme a total filter
            
        '''

        # parameters input array
        self.input_height = input_height
        self.input_width = input_width
        self.input_depth = input_depth

        # parameters filter
        self.filter_height = filter_height
        self.filter_width = filter_width
        self.filter_depth =  input_depth
        self.filter_number = filter_number


        # parameters output
        self.output_height = calculate_output_size(input_height, filter_height,zero_padding,stride)
        self.output_width = calculate_output_size(input_width, filter_width,zero_padding,stride)
        self.output_depth = filter_number

        #other parameters
        self.zero_padding = zero_padding
        self.stride = stride
        self.activators = activators
        self.learning_rate = learning_rate

        # make up of filter
        self.filter_total = []
        for i in range(filter_number):
            filter_total.append(Filter(filter_height,filter_width,filter_depth))
```

**Filter**
主要用于保存权重w ，偏置b 和梯度变化和更新
权重、偏置更新：
在SGD里，由于有mini-batch的存在，权重和偏置的更新关系如下,这里m是mini_batch 的大小


![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/10.PNG)
由于在卷积层不存在mini-batch 所以m=1  weights 用-=的方式，不断更新，每次更新learning_rate 乘以 gradient_weight, bias 同理
```
class Filter(object):
    '''
    main purpose of Filter : save the values of weights and bias
    we initialize the matrice of weights with small numbers and the matrice of bias with 0
    

    '''
    def _init_(self,filter_height,filter_width,filter_depth):
        self.weights = np.random.uniform(-1e-4,1e-4,(filter_height,filter_width,filter_depth))

        self.bias = 0

        self.gradient_weight = np.zeros((filter_height,filter_width,filter_depth))

        self.gradient_bias = 0

    def get_weight():
        return self.weights

    def get_bias():
        return self.bias

    def upadate（):
        self.weights -=learning_rate* self.gradient_weight
        self.bias -=learning_rate*self.gradient_bias
```
在filter 代码里只保存了一个filter 的信息，而卷积核可能不止一个，所以在ConvoLayer 中用最后的一个for 循环把 Filter 类里面的每一个filter 组装起来

### 0的填充
注意区分二维和三维的情况
```
def zero_padding(self,input_array,zp):
    '''
    zp is the number of 0 which we want to add in each side of each line

    there are 3 conditions we need to consider about
    1. no need to add 0 return original array
    2. 2-dimension:
    3. 3-dimension
     
   '''

    if zp == 0:
        return input_array
    else:
        if input_array.ndim == 3:
            input_height = input_array.shape[2]
            input_width = input_array.shape[1]
            input_depth = input_array.shape[0]

            expand_array = np.zeros((input_depth,input_height+2*zp,input_weight+2*zp))
            expand_array[:,zp:zp+input_height,zp:zp+input_weight] = input_array
            return expand_array


            else:
                if input_array.ndim == 2:
                    input_height = input_array.shape[1]
                    input_width = input_array.shape[0]

                    expand_array = np.zeros((input_height+2*zp,input_width+2*zp))


expand_array = [zp:zp+input_height,zp:zp+input_width] = input_array
                    return expand_array
```
### 开始正向传递
还是这幅图：

![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/matrice1.gif)

这幅图实现了类似SGD 中的 wx+b 的运算，只是和SGD 不同的是，连接w 和 x 的不再是乘号，而是 卷积
同时，由于z=wx+b 并不是最终的输出，最终的输出是f(z)，也就是激励值a，所以实际上在卷积神经网络的正向传递中，公式如下

![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/11.PNG)


这里我们使用Relu 激励函数，而不是sigmoid函数，Relu 激励函数的图如下

![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/13.png)


所以，我们需要定义一个由z到a 的Relu 类，在这个类里，有正向传递激励的 Relu_forward 函数，也有反向传递的导数Relu_backward 函数

为什么需要反向传递的Relu_forward 函数，回忆SGD的反向传递，有这么一个式子：

![](https://github.com/WuFan1992/CNN-Convolutional-Neural-Network/blob/master/convolution%20layer/image-convolution%20layer/12.PNG)

代码如下：
```
# this class is used to calcul the forward and the backward of Relu function

class Relu(object):

    # forward function
    def Relu_forward(self,z):
        return max(0,z)


    # backward function
    def Relu_backward(self,output):
        return 1 if output > 0 else 0
 ```
 
 
 在计算从z到a 的过程当中，相当于是对output 矩阵里的每一个元素进行操作，所以，最好定义一个矩阵元素操作函数
 
 ```
 def treat_element(input_array,funcion_treate):
    for i in np.nditer(input_array,op_flags=['readwrite']):
        i[...]= function_treate(i)
  ```
  
  如何理解这个函数，nditer 是一个迭代器，然后我们针对input_array里面的每一个元素， 使用function_treate 函数进行操作。
  
  到这里我们就可以开始正向传递
  
  ```
  # forward of convolution layer

def feedforward(self, input_array):
    
    self.input_array = input_array
    expand_input = zero_padding(input_array,self.zero_padding)

    for f in range(self.filter_number):

        filter_forward = self.filter_total[f] 

        conv_2d(expand_input,filter_forward.get_weight(),self.output_array[f],self.stride,self.zero_padding,self.filter_forward.get_bias())

    treate_element(self.output_array,self.activator.Relu_forward)
  ```
